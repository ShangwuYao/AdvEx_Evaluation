{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining cleverhans from git+http://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
      "  Updating ./src/cleverhans clone\n",
      "Requirement already satisfied: nose in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from cleverhans)\n",
      "Requirement already satisfied: pycodestyle in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from cleverhans)\n",
      "Requirement already satisfied: scipy in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from cleverhans)\n",
      "Requirement already satisfied: matplotlib in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from cleverhans)\n",
      "Requirement already satisfied: numpy>=1.8.2 in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scipy->cleverhans)\n",
      "Requirement already satisfied: pytz in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib->cleverhans)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib->cleverhans)\n",
      "Requirement already satisfied: six>=1.10 in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib->cleverhans)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib->cleverhans)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in ./anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib->cleverhans)\n",
      "Installing collected packages: cleverhans\n",
      "  Found existing installation: cleverhans 2.0.0\n",
      "    Uninstalling cleverhans-2.0.0:\n",
      "      Successfully uninstalled cleverhans-2.0.0\n",
      "  Running setup.py develop for cleverhans\n",
      "Successfully installed cleverhans\n"
     ]
    }
   ],
   "source": [
    "!pip install -e git+http://github.com/tensorflow/cleverhans.git#egg=cleverhans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/t10k-labels-idx1-ubyte.gz\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "X_test shape: (10000, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-02-27 02:12:52,458 cleverhans] Epoch 0 took 151.76362371444702 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on legitimate examples: 0.9884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-02-27 02:15:27,199 cleverhans] Epoch 1 took 151.71905827522278 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on legitimate examples: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-02-27 02:18:01,984 cleverhans] Epoch 2 took 151.76055145263672 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on legitimate examples: 0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-02-27 02:20:36,494 cleverhans] Epoch 3 took 151.49607014656067 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on legitimate examples: 0.9913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-02-27 02:23:11,211 cleverhans] Epoch 4 took 151.69823384284973 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on legitimate examples: 0.9925\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import flags\n",
    "import logging\n",
    "\n",
    "from cleverhans.utils_mnist import data_mnist\n",
    "from cleverhans.utils_tf import model_train, model_eval\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans_tutorials.tutorial_models import make_basic_cnn\n",
    "from cleverhans.utils import AccuracyReport, set_log_level\n",
    "from cleverhans.attacks import BasicIterativeMethod\n",
    "\n",
    "import os\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def mnist_tutorial(train_start=0, train_end=60000, test_start=0,\n",
    "                   test_end=10000, nb_epochs=6, batch_size=128,\n",
    "                   learning_rate=0.001,\n",
    "                   clean_train=True,\n",
    "                   testing=False,\n",
    "                   backprop_through_attack=False,\n",
    "                   nb_filters=64, num_threads=None):\n",
    "    \"\"\"\n",
    "    MNIST cleverhans tutorial\n",
    "    :param train_start: index of first training set example\n",
    "    :param train_end: index of last training set example\n",
    "    :param test_start: index of first test set example\n",
    "    :param test_end: index of last test set example\n",
    "    :param nb_epochs: number of epochs to train model\n",
    "    :param batch_size: size of training batches\n",
    "    :param learning_rate: learning rate for training\n",
    "    :param clean_train: perform normal training on clean examples only\n",
    "                        before performing adversarial training.\n",
    "    :param testing: if true, complete an AccuracyReport for unit tests\n",
    "                    to verify that performance is adequate\n",
    "    :param backprop_through_attack: If True, backprop through adversarial\n",
    "                                    example construction process during\n",
    "                                    adversarial training.\n",
    "    :param clean_train: if true, train on clean examples\n",
    "    :return: an AccuracyReport object\n",
    "    \"\"\"\n",
    "\n",
    "    # Object used to keep track of (and return) key accuracies\n",
    "    report = AccuracyReport()\n",
    "\n",
    "    # Set TF random seed to improve reproducibility\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "    # Set logging level to see debug information\n",
    "    set_log_level(logging.DEBUG)\n",
    "\n",
    "    # Create TF session\n",
    "    if num_threads:\n",
    "        config_args = dict(intra_op_parallelism_threads=1)\n",
    "    else:\n",
    "        config_args = {}\n",
    "    sess = tf.Session(config=tf.ConfigProto(**config_args))\n",
    "\n",
    "    # Get MNIST test data\n",
    "    X_train, Y_train, X_test, Y_test = data_mnist(train_start=train_start,\n",
    "                                                  train_end=train_end,\n",
    "                                                  test_start=test_start,\n",
    "                                                  test_end=test_end)\n",
    "\n",
    "    # Use label smoothing\n",
    "    assert Y_train.shape[1] == 10\n",
    "    label_smooth = .1\n",
    "    Y_train = Y_train.clip(label_smooth / 9., 1. - label_smooth)\n",
    "\n",
    "    # Define input TF placeholder\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "    model_path = \"models/mnist\"\n",
    "    # Train an MNIST model\n",
    "    train_params = {\n",
    "        'nb_epochs': nb_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "    fgsm_params = {'eps': 0.3,\n",
    "                   'clip_min': 0.,\n",
    "                   'clip_max': 1.}\n",
    "    rng = np.random.RandomState([2017, 8, 30])\n",
    "\n",
    "    if clean_train:\n",
    "        model = make_basic_cnn(nb_filters=nb_filters)\n",
    "        preds = model.get_probs(x)\n",
    "\n",
    "        def evaluate():\n",
    "            # Evaluate the accuracy of the MNIST model on legitimate test\n",
    "            # examples\n",
    "            eval_params = {'batch_size': batch_size}\n",
    "            acc = model_eval(\n",
    "                sess, x, y, preds, X_test, Y_test, args=eval_params)\n",
    "            report.clean_train_clean_eval = acc\n",
    "            assert X_test.shape[0] == test_end - test_start, X_test.shape\n",
    "            print('Test accuracy on legitimate examples: %0.4f' % acc)\n",
    "        model_train(sess, x, y, preds, X_train, Y_train, evaluate=evaluate,\n",
    "                    args=train_params, rng=rng)\n",
    "\n",
    "        # Calculate training error\n",
    "        if testing:\n",
    "            eval_params = {'batch_size': batch_size}\n",
    "            acc = model_eval(\n",
    "                sess, x, y, preds, X_train, Y_train, args=eval_params)\n",
    "            report.train_clean_train_clean_eval = acc\n",
    "\n",
    "        # Initialize the Fast Gradient Sign Method (FGSM) attack object and\n",
    "        # graph\n",
    "#         fgsm = FastGradientMethod(model, sess=sess)\n",
    "#         adv_x = fgsm.generate(x, **fgsm_params)\n",
    "        \n",
    "        bim = BasicIterativeMethod(model)\n",
    "        bim_params = {'eps': 0.3, 'clip_min': 0., 'clip_max': 1.,\n",
    "                      'nb_iter': 50,\n",
    "                      'eps_iter': .01}\n",
    "        adv_x = bim.generate(x, **bim_params)\n",
    "        preds_adv = model.get_probs(adv_x)\n",
    "\n",
    "        # Evaluate the accuracy of the MNIST model on adversarial examples\n",
    "        eval_par = {'batch_size': batch_size}\n",
    "        acc = model_eval(sess, x, y, preds_adv, X_test, Y_test, args=eval_par)\n",
    "        print('Test accuracy on adversarial examples: %0.4f\\n' % acc)\n",
    "        report.clean_train_adv_eval = acc\n",
    "\n",
    "        # Calculate training error\n",
    "        if testing:\n",
    "            eval_par = {'batch_size': batch_size}\n",
    "            acc = model_eval(sess, x, y, preds_adv, X_train,\n",
    "                             Y_train, args=eval_par)\n",
    "            report.train_clean_train_adv_eval = acc\n",
    "\n",
    "        print(\"Repeating the process, using adversarial training\")\n",
    "    # Redefine TF model graph\n",
    "    model_2 = make_basic_cnn(nb_filters=nb_filters)\n",
    "    preds_2 = model_2(x)\n",
    "#     fgsm2 = FastGradientMethod(model_2, sess=sess)\n",
    "#     adv_x_2 = fgsm2.generate(x, **fgsm_params)\n",
    "    \n",
    "    \n",
    "    bim2 = BasicIterativeMethod(model)\n",
    "    bim_params = {'eps': 0.3, 'clip_min': 0., 'clip_max': 1.,\n",
    "                  'nb_iter': 50,\n",
    "                  'eps_iter': .01}\n",
    "    adv_x_2 = bim.generate(x, **bim_params)\n",
    "    if not backprop_through_attack:\n",
    "        # For the fgsm attack used in this tutorial, the attack has zero\n",
    "        # gradient so enabling this flag does not change the gradient.\n",
    "        # For some other attacks, enabling this flag increases the cost of\n",
    "        # training, but gives the defender the ability to anticipate how\n",
    "        # the atacker will change their strategy in response to updates to\n",
    "        # the defender's parameters.\n",
    "        adv_x_2 = tf.stop_gradient(adv_x_2)\n",
    "    preds_2_adv = model_2(adv_x_2)\n",
    "\n",
    "    def evaluate_2():\n",
    "        # Accuracy of adversarially trained model on legitimate test inputs\n",
    "        eval_params = {'batch_size': batch_size}\n",
    "        accuracy = model_eval(sess, x, y, preds_2, X_test, Y_test,\n",
    "                              args=eval_params)\n",
    "        print('Test accuracy on legitimate examples: %0.4f' % accuracy)\n",
    "        report.adv_train_clean_eval = accuracy\n",
    "\n",
    "        # Accuracy of the adversarially trained model on adversarial examples\n",
    "        accuracy = model_eval(sess, x, y, preds_2_adv, X_test,\n",
    "                              Y_test, args=eval_params)\n",
    "        print('Test accuracy on adversarial examples: %0.4f' % accuracy)\n",
    "        report.adv_train_adv_eval = accuracy\n",
    "\n",
    "    # Perform and evaluate adversarial training\n",
    "    model_train(sess, x, y, preds_2, X_train, Y_train,\n",
    "                predictions_adv=preds_2_adv, evaluate=evaluate_2,\n",
    "                args=train_params, rng=rng)\n",
    "\n",
    "    # Calculate training errors\n",
    "    if testing:\n",
    "        eval_params = {'batch_size': batch_size}\n",
    "        accuracy = model_eval(sess, x, y, preds_2, X_train, Y_train,\n",
    "                              args=eval_params)\n",
    "        report.train_adv_train_clean_eval = accuracy\n",
    "        accuracy = model_eval(sess, x, y, preds_2_adv, X_train,\n",
    "                              Y_train, args=eval_params)\n",
    "        report.train_adv_train_adv_eval = accuracy\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    mnist_tutorial(nb_epochs=FLAGS.nb_epochs, batch_size=FLAGS.batch_size,\n",
    "                   learning_rate=FLAGS.learning_rate,\n",
    "                   clean_train=FLAGS.clean_train,\n",
    "                   backprop_through_attack=FLAGS.backprop_through_attack,\n",
    "                   nb_filters=FLAGS.nb_filters)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_integer('nb_filters', 64, 'Model size multiplier')\n",
    "    flags.DEFINE_integer('nb_epochs', 6, 'Number of epochs to train model')\n",
    "    flags.DEFINE_integer('batch_size', 128, 'Size of training batches')\n",
    "    flags.DEFINE_float('learning_rate', 0.001, 'Learning rate for training')\n",
    "    flags.DEFINE_bool('clean_train', True, 'Train on clean examples')\n",
    "    flags.DEFINE_bool('backprop_through_attack', False,\n",
    "                      ('If True, backprop through adversarial example '\n",
    "                       'construction process during adversarial training'))\n",
    "\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
